"""
Module ƒë·ªÉ t√¨m v√† qu·∫£n l√Ω c√°c dataset c√≥ s·∫µn trong h·ªá th·ªëng
"""
import os
import glob
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import json

class DatasetFinder:
    """
    Class ƒë·ªÉ t√¨m v√† qu·∫£n l√Ω c√°c dataset c√≥ s·∫µn
    """
    
    def __init__(self, base_paths: List[str] = None):
        """
        Kh·ªüi t·∫°o DatasetFinder
        
        Args:
            base_paths: Danh s√°ch c√°c ƒë∆∞·ªùng d·∫´n c∆° s·ªü ƒë·ªÉ t√¨m dataset
        """
        if base_paths is None:
            # C√°c ƒë∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh ƒë·ªÉ t√¨m dataset
            self.base_paths = [
                r'D:\AI',
                r'D:\Dataset',
                r'D:\Datasets',
                r'C:\Dataset',
                r'C:\Datasets',
                r'./dataset',
                r'./datasets',
                r'./data'
            ]
        else:
            self.base_paths = base_paths
            
        # C√°c ƒë·ªãnh d·∫°ng ·∫£nh ƒë∆∞·ª£c h·ªó tr·ª£
        self.image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.gif', '.webp']
        
        # C√°c t√™n th∆∞ m·ª•c c√≥ th·ªÉ ch·ª©a dataset
        self.dataset_keywords = ['dataset', 'datasets', 'data', 'images', 'train', 'test', 'val', 'validation']
    
    def find_datasets(self, min_images_per_class: int = 1) -> Dict[str, Dict]:
        """
        T√¨m t·∫•t c·∫£ c√°c dataset c√≥ s·∫µn trong h·ªá th·ªëng
        
        Args:
            min_images_per_class: S·ªë ·∫£nh t·ªëi thi·ªÉu m·ªói class ƒë·ªÉ ƒë∆∞·ª£c coi l√† dataset h·ª£p l·ªá
            
        Returns:
            Dictionary ch·ª©a th√¥ng tin c√°c dataset t√¨m ƒë∆∞·ª£c
        """
        datasets = {}
        
        print("üîç ƒêang t√¨m ki·∫øm datasets trong h·ªá th·ªëng...")
        print("=" * 60)
        
        for base_path in self.base_paths:
            if os.path.exists(base_path):
                print(f"üìÅ Qu√©t th∆∞ m·ª•c: {base_path}")
                found_datasets = self._scan_directory(base_path, min_images_per_class)
                datasets.update(found_datasets)
            else:
                print(f"‚ö†Ô∏è Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {base_path}")
        
        print(f"\n‚úÖ T√¨m th·∫•y {len(datasets)} dataset(s) h·ª£p l·ªá")
        return datasets
    
    def _scan_directory(self, directory: str, min_images_per_class: int) -> Dict[str, Dict]:
        """
        Qu√©t m·ªôt th∆∞ m·ª•c ƒë·ªÉ t√¨m dataset
        
        Args:
            directory: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c c·∫ßn qu√©t
            min_images_per_class: S·ªë ·∫£nh t·ªëi thi·ªÉu m·ªói class
            
        Returns:
            Dictionary ch·ª©a th√¥ng tin dataset t√¨m ƒë∆∞·ª£c
        """
        datasets = {}
        
        try:
            # Qu√©t t·∫•t c·∫£ th∆∞ m·ª•c con
            for item in os.listdir(directory):
                item_path = os.path.join(directory, item)
                
                if os.path.isdir(item_path):
                    # Ki·ªÉm tra xem th∆∞ m·ª•c n√†y c√≥ ph·∫£i l√† dataset kh√¥ng
                    dataset_info = self._analyze_potential_dataset(item_path, min_images_per_class)
                    
                    if dataset_info:
                        # T·∫°o t√™n dataset duy nh·∫•t
                        dataset_name = f"{os.path.basename(directory)}_{item}"
                        datasets[dataset_name] = dataset_info
                        print(f"  ‚úÖ T√¨m th·∫•y dataset: {dataset_name}")
                        print(f"     üìä Classes: {dataset_info['num_classes']}, Images: {dataset_info['total_images']}")
                    
                    # Qu√©t s√¢u h∆°n v√†o c√°c th∆∞ m·ª•c con
                    sub_datasets = self._scan_directory(item_path, min_images_per_class)
                    datasets.update(sub_datasets)
                    
        except PermissionError:
            print(f"  ‚ùå Kh√¥ng c√≥ quy·ªÅn truy c·∫≠p: {directory}")
        except Exception as e:
            print(f"  ‚ö†Ô∏è L·ªói khi qu√©t {directory}: {str(e)}")
        
        return datasets
    
    def _analyze_potential_dataset(self, path: str, min_images_per_class: int) -> Optional[Dict]:
        """
        Ph√¢n t√≠ch m·ªôt th∆∞ m·ª•c ƒë·ªÉ xem c√≥ ph·∫£i l√† dataset kh√¥ng
        
        Args:
            path: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c c·∫ßn ph√¢n t√≠ch
            min_images_per_class: S·ªë ·∫£nh t·ªëi thi·ªÉu m·ªói class
            
        Returns:
            Th√¥ng tin dataset n·∫øu h·ª£p l·ªá, None n·∫øu kh√¥ng
        """
        try:
            # Ki·ªÉm tra xem c√≥ ph·∫£i l√† c·∫•u tr√∫c dataset kh√¥ng (th∆∞ m·ª•c con ch·ª©a ·∫£nh)
            subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]
            
            if not subdirs:
                return None
            
            # Ph√¢n t√≠ch t·ª´ng th∆∞ m·ª•c con (class)
            class_info = {}
            total_images = 0
            valid_classes = 0
            
            for subdir in subdirs:
                subdir_path = os.path.join(path, subdir)
                image_count = self._count_images_in_directory(subdir_path)
                
                if image_count >= min_images_per_class:
                    class_info[subdir] = image_count
                    total_images += image_count
                    valid_classes += 1
            
            # Ch·ªâ coi l√† dataset n·∫øu c√≥ √≠t nh·∫•t 2 class v√† t·ªïng c·ªông c√≥ ·∫£nh
            if valid_classes >= 2 and total_images > 0:
                return {
                    'path': path,
                    'name': os.path.basename(path),
                    'num_classes': valid_classes,
                    'total_images': total_images,
                    'class_distribution': class_info,
                    'avg_images_per_class': total_images / valid_classes,
                    'min_images_per_class': min(class_info.values()) if class_info else 0,
                    'max_images_per_class': max(class_info.values()) if class_info else 0,
                    'is_balanced': self._is_balanced_dataset(class_info),
                    'suitable_for_fewshot': self._is_suitable_for_fewshot(class_info, min_images_per_class)
                }
            
        except Exception as e:
            pass  # B·ªè qua l·ªói v√† ti·∫øp t·ª•c
        
        return None
    
    def _count_images_in_directory(self, directory: str) -> int:
        """
        ƒê·∫øm s·ªë ·∫£nh trong m·ªôt th∆∞ m·ª•c
        
        Args:
            directory: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c
            
        Returns:
            S·ªë l∆∞·ª£ng ·∫£nh
        """
        count = 0
        try:
            for file in os.listdir(directory):
                if os.path.isfile(os.path.join(directory, file)):
                    file_lower = file.lower()
                    if any(file_lower.endswith(ext) for ext in self.image_extensions):
                        count += 1
        except:
            pass
        return count
    
    def _is_balanced_dataset(self, class_distribution: Dict[str, int]) -> bool:
        """
        Ki·ªÉm tra xem dataset c√≥ c√¢n b·∫±ng kh√¥ng
        
        Args:
            class_distribution: Ph√¢n b·ªë s·ªë ·∫£nh theo class
            
        Returns:
            True n·∫øu dataset c√¢n b·∫±ng
        """
        if not class_distribution:
            return False
        
        counts = list(class_distribution.values())
        avg_count = sum(counts) / len(counts)
        
        # Coi l√† c√¢n b·∫±ng n·∫øu t·∫•t c·∫£ class c√≥ s·ªë ·∫£nh trong kho·∫£ng 50%-150% c·ªßa trung b√¨nh
        for count in counts:
            if count < avg_count * 0.5 or count > avg_count * 1.5:
                return False
        
        return True
    
    def _is_suitable_for_fewshot(self, class_distribution: Dict[str, int], min_images_per_class: int) -> bool:
        """
        Ki·ªÉm tra xem dataset c√≥ ph√π h·ª£p cho few-shot learning kh√¥ng
        
        Args:
            class_distribution: Ph√¢n b·ªë s·ªë ·∫£nh theo class
            min_images_per_class: S·ªë ·∫£nh t·ªëi thi·ªÉu m·ªói class
            
        Returns:
            True n·∫øu ph√π h·ª£p cho few-shot learning
        """
        if not class_distribution:
            return False
        
        # C·∫ßn √≠t nh·∫•t 5 class v√† m·ªói class c√≥ √≠t nh·∫•t min_images_per_class ·∫£nh
        if len(class_distribution) < 5:
            return False
        
        for count in class_distribution.values():
            if count < min_images_per_class:
                return False
        
        return True
    
    def display_datasets(self, datasets: Dict[str, Dict], show_details: bool = True):
        """
        Hi·ªÉn th·ªã danh s√°ch c√°c dataset t√¨m ƒë∆∞·ª£c
        
        Args:
            datasets: Dictionary ch·ª©a th√¥ng tin dataset
            show_details: C√≥ hi·ªÉn th·ªã chi ti·∫øt kh√¥ng
        """
        if not datasets:
            print("‚ùå Kh√¥ng t√¨m th·∫•y dataset n√†o!")
            return
        
        print(f"\nüìã DANH S√ÅCH DATASET T√åM ƒê∆Ø·ª¢C ({len(datasets)} dataset):")
        print("=" * 80)
        
        for i, (name, info) in enumerate(datasets.items(), 1):
            print(f"\n{i}. {name}")
            print(f"   üìÅ ƒê∆∞·ªùng d·∫´n: {info['path']}")
            print(f"   üìä S·ªë class: {info['num_classes']}")
            print(f"   üñºÔ∏è T·ªïng ·∫£nh: {info['total_images']:,}")
            print(f"   üìà Trung b√¨nh ·∫£nh/class: {info['avg_images_per_class']:.1f}")
            print(f"   üìä Min/Max ·∫£nh/class: {info['min_images_per_class']}/{info['max_images_per_class']}")
            print(f"   ‚öñÔ∏è C√¢n b·∫±ng: {'‚úÖ C√≥' if info['is_balanced'] else '‚ùå Kh√¥ng'}")
            print(f"   üéØ Ph√π h·ª£p Few-Shot: {'‚úÖ C√≥' if info['suitable_for_fewshot'] else '‚ùå Kh√¥ng'}")
            
            if show_details and info['class_distribution']:
                print(f"   üìã Chi ti·∫øt class:")
                for class_name, count in sorted(info['class_distribution'].items()):
                    print(f"      ‚Ä¢ {class_name}: {count} ·∫£nh")
    
    def get_dataset_by_path(self, datasets: Dict[str, Dict], path: str) -> Optional[Dict]:
        """
        L·∫•y th√¥ng tin dataset theo ƒë∆∞·ªùng d·∫´n
        
        Args:
            datasets: Dictionary ch·ª©a th√¥ng tin dataset
            path: ƒê∆∞·ªùng d·∫´n dataset c·∫ßn t√¨m
            
        Returns:
            Th√¥ng tin dataset n·∫øu t√¨m th·∫•y
        """
        for dataset_info in datasets.values():
            if dataset_info['path'] == path:
                return dataset_info
        return None
    
    def save_datasets_info(self, datasets: Dict[str, Dict], output_file: str = "datasets_info.json"):
        """
        L∆∞u th√¥ng tin dataset v√†o file JSON
        
        Args:
            datasets: Dictionary ch·ª©a th√¥ng tin dataset
            output_file: T√™n file output
        """
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(datasets, f, indent=2, ensure_ascii=False)
            print(f"üíæ ƒê√£ l∆∞u th√¥ng tin dataset v√†o: {output_file}")
        except Exception as e:
            print(f"‚ùå L·ªói khi l∆∞u file: {str(e)}")
    
    def load_datasets_info(self, input_file: str = "datasets_info.json") -> Dict[str, Dict]:
        """
        Load th√¥ng tin dataset t·ª´ file JSON
        
        Args:
            input_file: T√™n file input
            
        Returns:
            Dictionary ch·ª©a th√¥ng tin dataset
        """
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                datasets = json.load(f)
            print(f"üìÇ ƒê√£ load th√¥ng tin dataset t·ª´: {input_file}")
            return datasets
        except Exception as e:
            print(f"‚ùå L·ªói khi load file: {str(e)}")
            return {}

def find_and_display_datasets(min_images_per_class: int = 1, show_details: bool = True) -> Dict[str, Dict]:
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ t√¨m v√† hi·ªÉn th·ªã datasets
    
    Args:
        min_images_per_class: S·ªë ·∫£nh t·ªëi thi·ªÉu m·ªói class
        show_details: C√≥ hi·ªÉn th·ªã chi ti·∫øt kh√¥ng
        
    Returns:
        Dictionary ch·ª©a th√¥ng tin dataset
    """
    finder = DatasetFinder()
    datasets = finder.find_datasets(min_images_per_class)
    finder.display_datasets(datasets, show_details)
    return datasets

if __name__ == "__main__":
    # Test ch·ª©c nƒÉng
    datasets = find_and_display_datasets(min_images_per_class=5, show_details=True)
    
    if datasets:
        # L∆∞u th√¥ng tin dataset
        finder = DatasetFinder()
        finder.save_datasets_info(datasets)
