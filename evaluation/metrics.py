"""
Module cho evaluation metrics
"""
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report

def calculate_detailed_metrics(predictions, targets, n_classes):
    """
    T√≠nh to√°n c√°c metrics chi ti·∫øt: precision, recall, F1-score, confusion matrix
    """
    # T√≠nh precision, recall, F1-score cho t·ª´ng class
    precision, recall, f1, support = precision_recall_fscore_support(
        targets, predictions, average=None, zero_division=0
    )
    
    # T√≠nh macro v√† weighted averages
    macro_precision = precision.mean()
    macro_recall = recall.mean()
    macro_f1 = f1.mean()
    
    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(
        targets, predictions, average='weighted', zero_division=0
    )
    
    # Confusion matrix
    cm = confusion_matrix(targets, predictions, labels=range(n_classes))
    
    # Classification report
    class_report = classification_report(targets, predictions, output_dict=True, zero_division=0)
    
    return {
        'precision_per_class': precision,
        'recall_per_class': recall,
        'f1_per_class': f1,
        'support_per_class': support,
        'macro_precision': macro_precision,
        'macro_recall': macro_recall,
        'macro_f1': macro_f1,
        'weighted_precision': weighted_precision,
        'weighted_recall': weighted_recall,
        'weighted_f1': weighted_f1,
        'confusion_matrix': cm,
        'classification_report': class_report
    }

def analyze_imbalance_impact(metrics, class_names):
    """
    Ph√¢n t√≠ch ·∫£nh h∆∞·ªüng c·ªßa dataset imbalance
    """
    print("\n‚öñÔ∏è PH√ÇN T√çCH ·∫¢NH H∆Ø·ªûNG IMBALANCE:")
    print("-" * 40)
    
    # L·∫•y support (s·ªë l∆∞·ª£ng samples) cho t·ª´ng class
    support_per_class = metrics['support_per_class']
    f1_per_class = metrics['f1_per_class']
    
    # T√≠nh to√°n imbalance ratio
    max_support = max(support_per_class)
    min_support = min(support_per_class)
    imbalance_ratio = min_support / max_support
    
    print(f"   Imbalance Ratio: {imbalance_ratio:.3f}")
    
    # Ph√¢n lo·∫°i classes theo s·ªë l∆∞·ª£ng samples
    minority_classes = []
    majority_classes = []
    
    for i, support in enumerate(support_per_class):
        if support <= max_support * 0.3:  # Classes c√≥ √≠t h∆°n 30% samples so v·ªõi class l·ªõn nh·∫•t
            minority_classes.append((class_names[i], support, f1_per_class[i]))
        else:
            majority_classes.append((class_names[i], support, f1_per_class[i]))
    
    # Ph√¢n t√≠ch performance
    if minority_classes:
        minority_f1_avg = np.mean([f1 for _, _, f1 in minority_classes])
        print(f"   Minority Classes ({len(minority_classes)}): {[name for name, _, _ in minority_classes]}")
        print(f"   Minority Classes F1-Score TB: {minority_f1_avg:.4f}")
    
    if majority_classes:
        majority_f1_avg = np.mean([f1 for _, _, f1 in majority_classes])
        print(f"   Majority Classes ({len(majority_classes)}): {[name for name, _, _ in majority_classes]}")
        print(f"   Majority Classes F1-Score TB: {majority_f1_avg:.4f}")
    
    # So s√°nh Macro vs Weighted
    macro_f1 = metrics['macro_f1']
    weighted_f1 = metrics['weighted_f1']
    f1_difference = weighted_f1 - macro_f1
    
    print(f"   Macro vs Weighted F1 Difference: {f1_difference:.4f}")
    
    # ƒê√°nh gi√° m·ª©c ƒë·ªô imbalance
    if imbalance_ratio > 0.8:
        balance_status = "C√¢n b·∫±ng t·ªët"
        impact_level = "Th·∫•p"
    elif imbalance_ratio > 0.5:
        balance_status = "C√¢n b·∫±ng trung b√¨nh"
        impact_level = "Trung b√¨nh"
    elif imbalance_ratio > 0.2:
        balance_status = "M·∫•t c√¢n b·∫±ng"
        impact_level = "Cao"
    else:
        balance_status = "M·∫•t c√¢n b·∫±ng nghi√™m tr·ªçng"
        impact_level = "R·∫•t cao"
    
    print(f"   Balance Status: {balance_status}")
    print(f"   Impact Level: {impact_level}")
    
    # C·∫£nh b√°o n·∫øu c√≥ v·∫•n ƒë·ªÅ
    if imbalance_ratio < 0.5:
        print(f"   ‚ö†Ô∏è C·∫¢NH B√ÅO: Dataset m·∫•t c√¢n b·∫±ng nghi√™m tr·ªçng!")
        print(f"      - Classes √≠t ·∫£nh c√≥ th·ªÉ b·ªã d·ª± ƒëo√°n sai")
        print(f"      - K·∫øt qu·∫£ c√≥ th·ªÉ kh√¥ng ƒë·∫°i di·ªán cho to√†n b·ªô dataset")
        print(f"      - C·∫ßn xem x√©t data augmentation ho·∫∑c resampling")
    
    if f1_difference > 0.1:
        print(f"   ‚ö†Ô∏è C·∫¢NH B√ÅO: Ch√™nh l·ªách Macro-Weighted F1 l·ªõn!")
        print(f"      - M√¥ h√¨nh thi√™n v·ªÅ classes nhi·ªÅu ·∫£nh")
        print(f"      - C·∫ßn c·∫£i thi·ªán performance cho minority classes")
    
    # G·ª£i √Ω c·∫£i thi·ªán
    if imbalance_ratio < 0.5:
        print(f"   üí° G·ª¢I √ù C·∫¢I THI·ªÜN:")
        print(f"      - TƒÉng data augmentation cho classes √≠t ·∫£nh")
        print(f"      - S·ª≠ d·ª•ng class weights trong loss function")
        print(f"      - Th·ª≠ nghi·ªám oversampling/undersampling")
        print(f"      - ƒêi·ªÅu ch·ªânh N_WAY ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·∫°i di·ªán ƒë·ªß classes")

def print_detailed_evaluation_metrics(metrics, class_names, dataset_name="Dataset"):
    """
    In ra c√°c metrics ƒë√°nh gi√° chi ti·∫øt
    """
    print(f"\nüìä ƒê√ÅNH GI√Å CHI TI·∫æT - {dataset_name.upper()}:")
    print("=" * 60)
    
    # Overall metrics
    print("üéØ METRICS T·ªîNG QUAN:")
    print(f"   Macro Precision: {metrics['macro_precision']:.4f}")
    print(f"   Macro Recall: {metrics['macro_recall']:.4f}")
    print(f"   Macro F1-Score: {metrics['macro_f1']:.4f}")
    print(f"   Weighted Precision: {metrics['weighted_precision']:.4f}")
    print(f"   Weighted Recall: {metrics['weighted_recall']:.4f}")
    print(f"   Weighted F1-Score: {metrics['weighted_f1']:.4f}")
    
    # Imbalance analysis
    analyze_imbalance_impact(metrics, class_names)
    
    print("\nüìà METRICS THEO T·ª™NG CLASS:")
    print("-" * 40)
    for i, class_name in enumerate(class_names):
        print(f"   {class_name}:")
        print(f"     Precision: {metrics['precision_per_class'][i]:.4f}")
        print(f"     Recall: {metrics['recall_per_class'][i]:.4f}")
        print(f"     F1-Score: {metrics['f1_per_class'][i]:.4f}")
        print(f"     Support: {metrics['support_per_class'][i]}")
    
    # Confusion matrix summary
    cm = metrics['confusion_matrix']
    print(f"\nüîç CONFUSION MATRIX SUMMARY:")
    print(f"   True Positives (TP): {np.sum(np.diag(cm))}")
    print(f"   False Positives (FP): {np.sum(cm) - np.sum(np.diag(cm))}")
    print(f"   Total Predictions: {np.sum(cm)}")
    
    # Class performance ranking
    f1_scores = metrics['f1_per_class']
    class_ranking = sorted(zip(class_names, f1_scores), key=lambda x: x[1], reverse=True)
    
    print(f"\nüèÜ X·∫æP H·∫†NG HI·ªÜU SU·∫§T THEO F1-SCORE:")
    for rank, (class_name, f1) in enumerate(class_ranking, 1):
        print(f"   {rank}. {class_name}: {f1:.4f}")
